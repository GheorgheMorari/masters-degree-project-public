services:
  llama-cpp:
    container_name: llama-cpp
    image: llama-cpp
    build:
      context: ./..
      dockerfile: infra/llama-cpp.Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - llm-net
    ports:
      - "8085:8085"
  rag-api:
    container_name: rag-api
    image: rag-api
    build:
      context: ./..
      dockerfile: infra/rag-api.Dockerfile
    networks:
      - llm-net
#    volumes:
#      - rag-api:/opt/masters_degree_project
    healthcheck:
      test: curl --fail http://localhost:8082/healthcheck || exit 1
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 10s
    ports:
      - "8086:8086"
networks:
  llm-net:
    driver: bridge

#volumes:
# rag-api:
#   external: false
